% Encoding: UTF-8

%%%
%
%Beim Erstellen der Bibtex-Datei wird empfohlen darauf zu achten, dass die DOI aufgeführt wird.
%
%%%

% !! DO NOT USE `label =   {ASF}` in other Misc entries !!


@Comment{jabref-meta: databaseType:biblatex;}


@misc{gu2023conceptgraphsopenvocabulary3dscene,
      title={ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning}, 
      author={Qiao Gu and Alihusein Kuwajerwala and Sacha Morin and Krishna Murthy Jatavallabhula and Bipasha Sen and Aditya Agarwal and Corban Rivera and William Paul and Kirsty Ellis and Rama Chellappa and Chuang Gan and Celso Miguel de Melo and Joshua B. Tenenbaum and Antonio Torralba and Florian Shkurti and Liam Paull},
      year={2023},
      eprint={2309.16650},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2309.16650}, 
}

@misc{ji2024arkitlabelmakernewscale,
      title={ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding}, 
      author={Guangda Ji and Silvan Weder and Francis Engelmann and Marc Pollefeys and Hermann Blum},
      year={2024},
      eprint={2410.13924},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.13924}, 
}

@inproceedings{armeni20193d,
  title={3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera},
  author={Armeni, Iro and He, Zhi-Yang and Gwak, JunYoung and Zamir, Amir R and Fischer, Martin and Malik, Jitendra and Savarese, Silvio},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5664--5673},
  year={2019}
}

@misc{koch2024open3dsgopenvocabulary3dscene,
      title={Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships}, 
      author={Sebastian Koch and Narunas Vaskevicius and Mirco Colosi and Pedro Hermosilla and Timo Ropinski},
      year={2024},
      eprint={2402.12259},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.12259}, 
}

@INPROCEEDINGS{8953305,
  author={Yang, Xu and Tang, Kaihua and Zhang, Hanwang and Cai, Jianfei},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Auto-Encoding Scene Graphs for Image Captioning}, 
  year={2019},
  volume={},
  number={},
  pages={10677-10686},
  keywords={Deep Learning;Vision + Language},
  doi={10.1109/CVPR.2019.01094}}
  
@inproceedings{schoenberger2016sfm,
    author={Sch\"{o}nberger, Johannes Lutz and Frahm, Jan-Michael},
    title={Structure-from-Motion Revisited},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016},
}

@inproceedings{schoenberger2016mvs,
    author={Sch\"{o}nberger, Johannes Lutz and Zheng, Enliang and Pollefeys, Marc and Frahm, Jan-Michael},
    title={Pixelwise View Selection for Unstructured Multi-View Stereo},
    booktitle={European Conference on Computer Vision (ECCV)},
    year={2016},
}

@InProceedings{Liu_2023_CVPR,
    author    = {Liu, Minghua and Zhu, Yinhao and Cai, Hong and Han, Shizhong and Ling, Zhan and Porikli, Fatih and Su, Hao},
    title     = {PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {21736-21746}
}

@InProceedings{10.1007/978-3-031-72652-1_25,
author="Kim, Hyunjin
and Sung, Minhyuk",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="PartSTAD: 2D-to-3D Part Segmentation Task Adaptation",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="422--439",
abstract="We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D segmentation lifting. Recent studies have highlighted the advantages of utilizing 2D segmentation models to achieve high-quality 3D segmentation through few-shot adaptation. However, previous approaches have focused on adapting 2D segmentation models for domain shift to rendered images and synthetic text descriptions, rather than optimizing the model specifically for 3D segmentation. Our proposed task adaptation method finetunes a 2D bounding box prediction model with an objective function for 3D segmentation. We introduce weights for 2D bounding boxes for adaptive merging and learn the weights using a small additional neural network. Additionally, we incorporate SAM, a foreground segmentation model on a bounding box, to improve the boundaries of 2D segments and consequently those of 3D segmentation. Our experiments on the PartNet-Mobility dataset show significant improvements with our task adaptation approach, achieving a 7.0{\%}p increase in mIoU and a 5.2{\%}p improvement in mAP50 for semantic and instance segmentation compared to the SotA few-shot 3D segmentation model. The code is available at https://github.com/KAIST-Visual-AI-Group/PartSTAD.",
isbn="978-3-031-72652-1"
}

@InProceedings{Mo_2019_CVPR,
    author = {Mo, Kaichun and Zhu, Shilin and Chang, Angel X. and Yi, Li and Tripathi, Subarna and Guibas, Leonidas J. and Su, Hao},
    title = {{PartNet}: A Large-Scale Benchmark for Fine-Grained and Hierarchical Part-Level {3D} Object Understanding},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019}
}

@inproceedings{delitzas2024scenefun3d, 
  title = {{SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes}}, 
  author = {Delitzas, Alexandros and Takmaz, Ayca and Tombari, Federico and Sumner, Robert and Pollefeys, Marc and Engelmann, Francis}, 
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  year = {2024}
}

@inproceedings{baruch2021arkitscenes,
title={{ARK}itScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile {RGB}-D Data},
author={Gilad Baruch and Zhuoyuan Chen and Afshin Dehghan and Yuri Feigin and Peter Fu and Thomas Gebauer and Daniel Kurz and Tal Dimry and Brandon Joffe and Arik Schwartz and Elad Shulman},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=tjZjv_qh_CE}
}

@misc{liu2024grounding3dsceneaffordance,
      title={Grounding 3D Scene Affordance From Egocentric Interactions}, 
      author={Cuiyu Liu and Wei Zhai and Yuhang Yang and Hongchen Luo and Sen Liang and Yang Cao and Zheng-Jun Zha},
      year={2024},
      eprint={2409.19650},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.19650}, 
}

@misc{takmaz2023openmask3dopenvocabulary3dinstance,
      title={OpenMask3D: Open-Vocabulary 3D Instance Segmentation}, 
      author={Ayça Takmaz and Elisabetta Fedele and Robert W. Sumner and Marc Pollefeys and Federico Tombari and Francis Engelmann},
      year={2023},
      eprint={2306.13631},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.13631}, 
}

@InProceedings{Nguyen_2024_CVPR,
    author    = {Nguyen, Phuc and Ngo, Tuan Duc and Kalogerakis, Evangelos and Gan, Chuang and Tran, Anh and Pham, Cuong and Nguyen, Khoi},
    title     = {Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {4018-4028}
}

@misc{huang2024openins3dsnaplookup3d,
      title={OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation}, 
      author={Zhening Huang and Xiaoyang Wu and Xi Chen and Hengshuang Zhao and Lei Zhu and Joan Lasenby},
      year={2024},
      eprint={2309.00616},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2309.00616}, 
}

@article{cheng2024yolow,
title={YOLO-World: Real-Time Open-Vocabulary Object Detection},
author={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
journal={arXiv preprint arXiv:2401.17270},
year={2024}
}

@INPROCEEDINGS{7298990,
  author={Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image retrieval using scene graphs}, 
  year={2015},
  volume={},
  number={},
  pages={3668-3678},
  keywords={Grounding;Semantics;Image retrieval;Visualization;Boats;Computational modeling;Context},
  doi={10.1109/CVPR.2015.7298990}}

@Article{Brin2023,
author={Brin, Dana
and Sorin, Vera
and Vaid, Akhil
and Soroush, Ali
and Glicksberg, Benjamin S.
and Charney, Alexander W.
and Nadkarni, Girish
and Klang, Eyal},
title={Comparing ChatGPT and GPT-4 performance in USMLE soft skill assessments},
journal={Scientific Reports},
year={2023},
month={Oct},
day={01},
volume={13},
number={1},
pages={16492},
issn={2045-2322},
doi={10.1038/s41598-023-43436-9},
url={https://doi.org/10.1038/s41598-023-43436-9}
}


  @misc{kirillov2023segment,
      title={Segment Anything}, 
      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
      year={2023},
      eprint={2304.02643},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.02643}, 
}
@misc{bommasani2022opportunitiesrisksfoundationmodels,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}, 
}

@misc{tripathi2019usingscenegraphcontext,
      title={Using Scene Graph Context to Improve Image Generation}, 
      author={Subarna Tripathi and Anahita Bhiwandiwalla and Alexei Bastidas and Hanlin Tang},
      year={2019},
      eprint={1901.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1901.03762}, 
}

@misc{johnson2018imagegenerationscenegraphs,
      title={Image Generation from Scene Graphs}, 
      author={Justin Johnson and Agrim Gupta and Li Fei-Fei},
      year={2018},
      eprint={1804.01622},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1804.01622}, 
}
@misc{zhu2022scenegraphgenerationcomprehensive,
      title={Scene Graph Generation: A Comprehensive Survey}, 
      author={Guangming Zhu and Liang Zhang and Youliang Jiang and Yixuan Dang and Haoran Hou and Peiyi Shen and Mingtao Feng and Xia Zhao and Qiguang Miao and Syed Afaq Ali Shah and Mohammed Bennamoun},
      year={2022},
      eprint={2201.00443},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.00443}, 
}
@Article{Sharma2022,
author={Sharma, Rabi
and Saqib, Muhammad
and Lin, C. T.
and Blumenstein, Michael},
title={A Survey on Object Instance Segmentation},
journal={SN Computer Science},
year={2022},
month={Sep},
day={29},
volume={3},
number={6},
pages={499},
abstract={In recent years, instance segmentation has become a key research area in computer vision. This technology has been applied in varied applications such as robotics, healthcare and intelligent driving. Instance segmentation technology not only detects the location of the object but also marks edges for each single instance, which can solve both object detection and semantic segmentation concurrently. Our survey will give a detail introduction to the instance segmentation technology based on deep learning, reinforcement learning and transformers. Further, we will discuss about its development in this field along with the most common datasets used. We will also focus on different challenges and future development scope for instance segmentation. This technology will provide a strong reference for future researchers in our survey paper.},
issn={2661-8907},
doi={10.1007/s42979-022-01407-3},
url={https://doi.org/10.1007/s42979-022-01407-3}
}

@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}

@article{Zhou2018,
    author    = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},
    title     = {{Open3D}: {A} Modern Library for {3D} Data Processing},
    journal   = {arXiv:1801.09847},
    year      = {2018},
}

@article{Pytorch_Pointnet_Pointnet2,
      author = {Xu Yan},
      title = {Pointnet/Pointnet++ Pytorch},
      url = {https://github.com/yanx27/Pointnet_Pointnet2_pytorch},
      year = {2019}
}

@book{gibson,
 author = {Gibson, J.J.}, 
 year = {2014},
 title = {The Ecological Approach to Visual Perception: Classic Edition (1st ed.).},
 journal = {Psychology Press.}, 
 doi = {https://doi.org/10.4324/9781315740218}
}

@article{mueller2022instant,
    author = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    journal = {ACM Trans. Graph.},
    issue_date = {July 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    pages = {102:1--102:15},
    articleno = {102},
    numpages = {15},
    url = {https://doi.org/10.1145/3528223.3530127},
    doi = {10.1145/3528223.3530127},
    publisher = {ACM},
    address = {New York, NY, USA},
}

@InProceedings{10.1007/978-3-031-20080-9_42,
author="Minderer, Matthias
and Gritsenko, Alexey
and Stone, Austin
and Neumann, Maxim
and Weissenborn, Dirk
and Dosovitskiy, Alexey
and Mahendran, Aravindh
and Arnab, Anurag
and Dehghani, Mostafa
and Shen, Zhuoran
and Wang, Xiao
and Zhai, Xiaohua
and Kipf, Thomas
and Houlsby, Neil",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Simple Open-Vocabulary Object Detection",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="728--755",
abstract="Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub github.com/google-research/scenic/tree/main/scenic/projects/owl{\_}vit.",
isbn="978-3-031-20080-9"
}