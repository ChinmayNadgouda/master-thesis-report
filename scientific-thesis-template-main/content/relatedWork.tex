In this chapter, we will have a look at the state-of-the-art research in the field of scene graph generation and part-object segmentation. We will have a look at various papers and divide this chapter into four sections. The four sections are 3D scene graph generation, part-object segmentation,
task-driven affordance grounding and open vocabulary object detection. Finally, we will note down the limitations in the current research and thus motivate our thesis to improve these limitations. Specifically, we try to improve fine-grained segmentation i.e. pat-object segmentation, also called functionality segmentation. The reason behind reviewing the current research is to avoid redundant research. It also helps in closing the gaps in domain knowledge by giving a summary of the relevant research. Additionally, literature review can spark innovation by realising
the limitations of current research.
 
 \section{3D scene graph generation}
On the subject of 3D scene graph generation, we will take a look at the following papers, 
Open3DSG\cite{koch2024open3dsgopenvocabulary3dscene}, 3D Scene Graph \cite{armeni20193d} and ConceptGraphs \cite{gu2023conceptgraphsopenvocabulary3dscene}. 
We look at these papers, as to take inspiration for building our own scene graph 
using either of these implementations. Our take on task two of the SceneFun3D, task-driven affordance grounding will 
take the help of the scene graph generated from one of these implementations. \\
To generate the scene graphs these papers utilise,
a sequence of RGB-D images and poses of the camera. RGB-D images are the output from a camera.
RGB expands to Red, Green and Blue, it is a color image and D stands for depth image. The color images
are used for object detection and the depth images along with the poses help in building the 3D point clouds. The poses are
the position and orientation of the camera in 3D. These poses can be obtained from the camera itself or by using Structure from Motion (SfM) software like
COLMAP \cite{schoenberger2016mvs} \cite{schoenberger2016sfm}. \\ Out of the three, ConceptGraphs and 
Open3DSG generate the 3D scene graph using open vocabulary object detection. Open vocabulary
object detection (OVOD) enables the model to detect unseen objects at runtime. ConceptGraph uses YOLO-World \cite{cheng2024yolow} for this task whereas Open3DSG uses 
various vision language models (VLMs) like CLIP \cite{radford2021learningtransferablevisualmodels} and Open Seg \cite{ghiasi2022scalingopenvocabularyimagesegmentation}.
 The scene graphs also include the relationships between the 
objects. The relationships can be either spatial or comparative. \\
Although these implementations are robust and state-of-the-art, they face the limitation of not detecting
parts of the individual object. They lack the fine-grained semantic relationships between the object and its part, for example, a door and its handle.
Such fine-grained information can be useful for manipulating various objects in the scene. This thesis will tackle the exact problem of 
finding fine-grained relationships between the object and its part.

 \section{Part-Object segmentation}
The task of segmenting out a part from a 3D object is still novel in today's research due to the limited availability of large-scale 3D datasets
, works like PartNet \cite{Mo_2019_CVPR}, and SceneFun3D \cite{delitzas2024scenefun3d} have made notable contributions by providing benchmarks and datasets for this problem. 
However, they lack in some areas, PartNet has a limited set of data and it is not specifically related to robotics or object manipulation, the parts are not
exactly the functional element of the parent object that can be manipulated by a robot. They have general
classes of objects such as aeroplanes and chairs to name a few. On the other hand, SceneFun3D has parts annotated as the functional elements in the entire scene. Considering
the entire scene as the query for segmenting a part as a functional element has some limitations and their results show an average of 7\% average precision, 18\% average precison for mIoU 
50\% and 26\% average precision for mIoU 25\%. We consider their results as a baseline and propose to improve the results. \\
We plan to use only the object as the query to which the part is attached and not the entire scene, thus hypothetically improving the results.
Part-object segmentation is an important task in the context of our thesis. Thus, we look at the current research to utilise one or more of these
in our final implementation. Works like \cite{Liu_2023_CVPR}, and \cite{10.1007/978-3-031-72652-1_25} perform part-object segmentation on 2D images using techniques such as LLMs, VLMs and CNNs and then transfer the 
segments from the 2D to 3D. The reason for not directly segmenting the 3D data is that they lack large-scale 3D datasets with such pre-annotated parts
and object point clouds. \\
However, in our work, we focus on the part-object segmentation of 3D point clouds directly. PointNet++ implementation provides a way to segment out parts from an object \cite{qi2017pointnetdeephierarchicalfeature}. 
They use the PartNet dataset which has objects and its part segmented to train their 3D CNN. Similarly, we plan on using the SceneFun3D dataset as well as 
the ARKit LabelMaker dataset \cite{ji2024arkitlabelmakernewscale}. The ARKit LabelMaker dataset provides object detections for each scene from the ARKitScenes dataset \cite{baruch2021arkitscenes}, whereas the SceneFun3D has
the parts annotated for around 700 scenes for the same ARKitScenes. We plan to merge the two datasets to get the final data which contains the objects as 
well as their parts annotated. We further plan to use this dataset to train PointNet ++ and Mask3D \cite{schult2023mask3dmasktransformer3d}.
 \section{Task-driven affordance grounding}
Given a specific query (task), identifying and associating the functional elements of an object to the task in the query is known as Task-driven affordance grounding. 
In the context of grounding 3D scene affordances to the given task, \citet{liu2024grounding3dsceneaffordance} propose
 Egocentric Interaction-driven 3D Scene Affordance Grounding (Ego-SAG). The paper proposes the identification of affordance regions in a 3D scene based on 
 an egocentric interaction video. The paper further compares their results with the following line of work,
\cite{huang2024openins3dsnaplookup3d}, \cite{Nguyen_2024_CVPR} and \cite{takmaz2023openmask3dopenvocabulary3dinstance}. However, the fine-grained 
segmentation of the functional element required to perform the task is absent. Open3DIS presents a framework for 3D instance segmentation which is based on
zero-shot learning for unseen objects. They leverage 2D masks to guide the segmentation process and integrate them with 3D feature extraction pipelines. And, in the context
of task-driven affordance grounding, the capabilities of Open3DIS are highly relevant but lack the depth of fine-grained segmentation. OpenMask3D is another framework
 for open-vocabulary 3D scene understanding. They employ
various foundation models to generalize various 3D tasks. A transformer-based backbone integrating 2D and 3D features to enhance recognition and segmentation tasks is leveraged. 
Therefore, tasks such as identifying objects capable of specific interactions benefit from OpenMask3D. 
But even OpenMask3D lacks fine-grained scene understanding at the functional element level.
OpenMask3D \cite{takmaz2023openmask3dopenvocabulary3dinstance} is also used for comparison by
\citet{delitzas2024scenefun3d}, where they modify the implementation of OpenMask3D a bit to incorporate segmentation of the functional elements. 
They describe this task in \cite{delitzas2024scenefun3d} and present their results and show notable improvements.\\
However, the results obtained by \citet{delitzas2024scenefun3d}
can be considered baseline results and further improved by utilising scene graphs as an intermediate step for task-driven affordance grounding.
\section{Open-vocabulary object detection}
Open Vocabulary Object Detection (OVOD) refers to the ability of a model to detect objects not seen during the training process. 
In short, the model can detect and label unseen objects at runtime. This is useful for real-world applications where annotations for all the 
existing objects in the world are not feasible or when there are constraints on resources whilst training. State-of-the-art open vocabulary object detections include
YOLO-World, an advanced, real-time Ultralytics YOLOv8-based approach, it enables the detection of any object in a given image based on descriptive texts \cite{cheng2024yolow}. 
Most of the OVOD methods use a text encoder  (e.g., CLIP, ALIGN \cite{jia2021scalingvisualvisionlanguagerepresentation}) and a object detection backbone (e.g., YOLO \cite{redmon2016lookonceunifiedrealtime}, Faster R-CNN \cite{ren2016fasterrcnnrealtimeobject}) to perform zero-shot predictions. The model takes
help of the text encoder such as CLIP to get text embeddings in a shared multimodal space from the image and later fuses it with the image features obtained from the 
object detection module. This fusion helps the model to regress bounding boxes and predict class labels based on the similarity between the image features and text embeddings.
In \cite{minderer2022simpleopenvocabularyobjectdetection}, the authors introduce OWL-ViT (Open World Learning with Vision Transformers) that use CLIP-like vision-text embeddings and a Vision Transformer backbone.
 A combination of these enables OWL-ViT  to detect objects from arbitray text descriptions. On a similar note, \citet{li2022groundedlanguageimagepretraining} contribute
 GLIP (Grounded Language-Image Pre-training), it leverages a visual encoder in conjunction with a text encoder to learn grounded vision-language representation.\\
In our implementation, we leverage ConceptGraphs to generate scene graphs. ConceptGraphs in turn uses YOLO-World to detect objects in realtime, thus
achieving open vocabulary obejct detection and scene graph generation. \\

 In this thesis, we mitigate all the abovementioned limitations and propose
a novel way to create scene graphs that have the ability to functionally segment
interactive elements and provide task-driven affordance grounding keeping in context the 
segmented fine-grained segmentation. At the core of our idea is the integration of ConceptGraphs with
a point cloud network which will segment the fine-grained part from its object. For this, we will train pre-existing 
models like PointNet ++ and Mask3D on our merged dataset obtained from merging SceneFun3D and ARKit LabelMaker datasets. 
Further, we will employ an Large Language Model (LLM) to assign captions with semantically rich descriptions and 
labels for each instance of the part segment obtained out of every object and store them for language querying. 
To summarise, our novel contributions are:
\begin{compactenum}[1.]
\item We propose a novel way to build scene graphs that includes fine-grained segmentation of objects and their parts by tackling the below two tasks.
\item We tackle the Task 1 functionality segmentation from the authors \citet{delitzas2024scenefun3d} and propose to improve their 
baseline results.
\item We also tackle the Task2 task-driven affordance grounding from the authors mentioned in the above point 
and propose to improve on the concept by leveraging scene graphs.
\end{compactenum}


