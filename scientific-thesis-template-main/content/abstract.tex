The ability of a robot to interact with its surroundings greatly benefits from a semantically rich scene graph. However, this requires the scene graph to have semantic information about the objects as well as their functional interactive elements, also known as parts. ConceptGraphs, a state-of-the-art piece of research, fulfills the first requirement but lacks the fine-grained segmentation of object parts needed to fulfill the second requirement. In this thesis, we introduce a novel extension to the implementation of ConceptGraphs. We propose to integrate Mask3D with ConceptGraphs. We train the Mask3D model on a dataset resulting from the merger of ARKit LabelMaker and SceneFun3D datasets. This model, capable of part-object segmentation, will enhance ConceptGraph's ability to capture fine-grained information about object parts. We also test how well our system works by doing the tasks of functionality segmentation and task-driven affordance grounding that are defined in SceneFun3D. We push forward the state-of-the-art performance on the SceneFun3D dataset with our implemented system, demonstrating the efficacy of our developed system. The results obtained show an increase of 9\% in Average Precision (AP) at mean Intersection over Union (mIoU) thresholds of 25\% and 50\% and over 12\% increase in AP,  the average over
different IoU thresholds from 0.5 to 0.95 with a step of 0.05.