We saw the limitations current scene graphs posses in terms of functionality segmentation of interactive elements. We also
saw the importance of having such fine grained understanding of a scene at interactive element level. Therefore, we propose a novel way of
extending the current implementaiton of \cite{ConceptGraphs} by adding functionality segmentation as a part of its process while generating the 
scene graph. For this, we plan on training point cloud neural networks as well as transformers on a dataset which has annotations for both the
object and its part. Such a dataset is one of its kind and will be formed by merging two datasets, scenefun3d and labelmaker ARKIT scenes.
The scenefun3D dataset has annotations for various functional interactive elements and categorises them into eight classes namely,
tip_push, hook_turn, hook_pull, pinch_pull, key_press, unplug, plug_in and rotate. The label maker dataset derives itself from ARKIT scenes dataset
by Apple, they build an automatic pipeline to segment out objects from the ARKIT scenes point clouds by first segmenting the obejcts in 2D and 
then lifting these segments to 3D. As both these datasets are in the same co-ordinate world we plan to merge them and carve out part-object
point clouds for training the neural networks and transformers. Later, we propose another innovative idea of using the scene graph for 
task-driven affordance grounding. For this, the scene graph will be given a language based task description and it will be expected to return
the interactive segment that can be used to complete the task. Generally, each functional interactive element is associated with an object.
We plan to retrieve this object by using the scene graph and later pass this object to the pre-trained neural network and Transformer to get
segmentation. In this segmentation we expect the obejct segmented out from its part, which will be the answer to the task description given in 
natural language.