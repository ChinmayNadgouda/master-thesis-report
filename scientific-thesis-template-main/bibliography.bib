% Encoding: UTF-8

%%%
%
%Beim Erstellen der Bibtex-Datei wird empfohlen darauf zu achten, dass die DOI aufgeführt wird.
%
%%%

% !! DO NOT USE `label =   {ASF}` in other Misc entries !!


@Comment{jabref-meta: databaseType:biblatex;}


@misc{gu2023conceptgraphsopenvocabulary3dscene,
      title={ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning}, 
      author={Qiao Gu and Alihusein Kuwajerwala and Sacha Morin and Krishna Murthy Jatavallabhula and Bipasha Sen and Aditya Agarwal and Corban Rivera and William Paul and Kirsty Ellis and Rama Chellappa and Chuang Gan and Celso Miguel de Melo and Joshua B. Tenenbaum and Antonio Torralba and Florian Shkurti and Liam Paull},
      year={2023},
      eprint={2309.16650},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2309.16650}, 
}

@misc{ji2024arkitlabelmakernewscale,
      title={ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding}, 
      author={Guangda Ji and Silvan Weder and Francis Engelmann and Marc Pollefeys and Hermann Blum},
      year={2024},
      eprint={2410.13924},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.13924}, 
}

@misc{ji2024arkitlabelmakernewscale,
      title={ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding}, 
      author={Guangda Ji and Silvan Weder and Francis Engelmann and Marc Pollefeys and Hermann Blum},
      year={2024},
      eprint={2410.13924},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.13924}, 
}

@inproceedings{armeni20193d,
  title={3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera},
  author={Armeni, Iro and He, Zhi-Yang and Gwak, JunYoung and Zamir, Amir R and Fischer, Martin and Malik, Jitendra and Savarese, Silvio},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5664--5673},
  year={2019}
}

@misc{koch2024open3dsgopenvocabulary3dscene,
      title={Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships}, 
      author={Sebastian Koch and Narunas Vaskevicius and Mirco Colosi and Pedro Hermosilla and Timo Ropinski},
      year={2024},
      eprint={2402.12259},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.12259}, 
}

@INPROCEEDINGS{8953305,
  author={Yang, Xu and Tang, Kaihua and Zhang, Hanwang and Cai, Jianfei},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Auto-Encoding Scene Graphs for Image Captioning}, 
  year={2019},
  volume={},
  number={},
  pages={10677-10686},
  keywords={Deep Learning;Vision + Language},
  doi={10.1109/CVPR.2019.01094}}
  

@INPROCEEDINGS{8953305,
  author={Yang, Xu and Tang, Kaihua and Zhang, Hanwang and Cai, Jianfei},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Auto-Encoding Scene Graphs for Image Captioning}, 
  year={2019},
  volume={},
  number={},
  pages={10677-10686},
  keywords={Deep Learning;Vision + Language},
  doi={10.1109/CVPR.2019.01094}}
  
@inproceedings{schoenberger2016sfm,
    author={Sch\"{o}nberger, Johannes Lutz and Frahm, Jan-Michael},
    title={Structure-from-Motion Revisited},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016},
}

@inproceedings{schoenberger2016mvs,
    author={Sch\"{o}nberger, Johannes Lutz and Zheng, Enliang and Pollefeys, Marc and Frahm, Jan-Michael},
    title={Pixelwise View Selection for Unstructured Multi-View Stereo},
    booktitle={European Conference on Computer Vision (ECCV)},
    year={2016},
}

@InProceedings{Liu_2023_CVPR,
    author    = {Liu, Minghua and Zhu, Yinhao and Cai, Hong and Han, Shizhong and Ling, Zhan and Porikli, Fatih and Su, Hao},
    title     = {PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {21736-21746}
}

@InProceedings{10.1007/978-3-031-72652-1_25,
author="Kim, Hyunjin
and Sung, Minhyuk",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="PartSTAD: 2D-to-3D Part Segmentation Task Adaptation",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="422--439",
abstract="We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D segmentation lifting. Recent studies have highlighted the advantages of utilizing 2D segmentation models to achieve high-quality 3D segmentation through few-shot adaptation. However, previous approaches have focused on adapting 2D segmentation models for domain shift to rendered images and synthetic text descriptions, rather than optimizing the model specifically for 3D segmentation. Our proposed task adaptation method finetunes a 2D bounding box prediction model with an objective function for 3D segmentation. We introduce weights for 2D bounding boxes for adaptive merging and learn the weights using a small additional neural network. Additionally, we incorporate SAM, a foreground segmentation model on a bounding box, to improve the boundaries of 2D segments and consequently those of 3D segmentation. Our experiments on the PartNet-Mobility dataset show significant improvements with our task adaptation approach, achieving a 7.0{\%}p increase in mIoU and a 5.2{\%}p improvement in mAP50 for semantic and instance segmentation compared to the SotA few-shot 3D segmentation model. The code is available at https://github.com/KAIST-Visual-AI-Group/PartSTAD.",
isbn="978-3-031-72652-1"
}

@InProceedings{Mo_2019_CVPR,
    author = {Mo, Kaichun and Zhu, Shilin and Chang, Angel X. and Yi, Li and Tripathi, Subarna and Guibas, Leonidas J. and Su, Hao},
    title = {{PartNet}: A Large-Scale Benchmark for Fine-Grained and Hierarchical Part-Level {3D} Object Understanding},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019}
}

@inproceedings{delitzas2024scenefun3d, 
  title = {{SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes}}, 
  author = {Delitzas, Alexandros and Takmaz, Ayca and Tombari, Federico and Sumner, Robert and Pollefeys, Marc and Engelmann, Francis}, 
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  year = {2024}
}

@inproceedings{baruch2021arkitscenes,
title={{ARK}itScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile {RGB}-D Data},
author={Gilad Baruch and Zhuoyuan Chen and Afshin Dehghan and Yuri Feigin and Peter Fu and Thomas Gebauer and Daniel Kurz and Tal Dimry and Brandon Joffe and Arik Schwartz and Elad Shulman},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=tjZjv_qh_CE}
}

@inproceedings{baruch2021arkitscenes,
title={{ARK}itScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile {RGB}-D Data},
author={Gilad Baruch and Zhuoyuan Chen and Afshin Dehghan and Yuri Feigin and Peter Fu and Thomas Gebauer and Daniel Kurz and Tal Dimry and Brandon Joffe and Arik Schwartz and Elad Shulman},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=tjZjv_qh_CE}
}

@misc{liu2024grounding3dsceneaffordance,
      title={Grounding 3D Scene Affordance From Egocentric Interactions}, 
      author={Cuiyu Liu and Wei Zhai and Yuhang Yang and Hongchen Luo and Sen Liang and Yang Cao and Zheng-Jun Zha},
      year={2024},
      eprint={2409.19650},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.19650}, 
}

@misc{schult2023mask3dmasktransformer3d,
      title={Mask3D: Mask Transformer for 3D Semantic Instance Segmentation}, 
      author={Jonas Schult and Francis Engelmann and Alexander Hermans and Or Litany and Siyu Tang and Bastian Leibe},
      year={2023},
      eprint={2210.03105},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2210.03105}, 
}

@misc{takmaz2023openmask3dopenvocabulary3dinstance,
      title={OpenMask3D: Open-Vocabulary 3D Instance Segmentation}, 
      author={Ayça Takmaz and Elisabetta Fedele and Robert W. Sumner and Marc Pollefeys and Federico Tombari and Francis Engelmann},
      year={2023},
      eprint={2306.13631},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.13631}, 
}

@InProceedings{Nguyen_2024_CVPR,
    author    = {Nguyen, Phuc and Ngo, Tuan Duc and Kalogerakis, Evangelos and Gan, Chuang and Tran, Anh and Pham, Cuong and Nguyen, Khoi},
    title     = {Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {4018-4028}
}

@misc{huang2024openins3dsnaplookup3d,
      title={OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation}, 
      author={Zhening Huang and Xiaoyang Wu and Xi Chen and Hengshuang Zhao and Lei Zhu and Joan Lasenby},
      year={2024},
      eprint={2309.00616},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2309.00616}, 
}

@article{cheng2024yolow,
title={YOLO-World: Real-Time Open-Vocabulary Object Detection},
author={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
journal={arXiv preprint arXiv:2401.17270},
year={2024}
}

@INPROCEEDINGS{7298990,
  author={Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image retrieval using scene graphs}, 
  year={2015},
  volume={},
  number={},
  pages={3668-3678},
  keywords={Grounding;Semantics;Image retrieval;Visualization;Boats;Computational modeling;Context},
  doi={10.1109/CVPR.2015.7298990}}

@Article{Brin2023,
author={Brin, Dana
and Sorin, Vera
and Vaid, Akhil
and Soroush, Ali
and Glicksberg, Benjamin S.
and Charney, Alexander W.
and Nadkarni, Girish
and Klang, Eyal},
title={Comparing ChatGPT and GPT-4 performance in USMLE soft skill assessments},
journal={Scientific Reports},
year={2023},
month={Oct},
day={01},
volume={13},
number={1},
pages={16492},
issn={2045-2322},
doi={10.1038/s41598-023-43436-9},
url={https://doi.org/10.1038/s41598-023-43436-9}
}

@misc{he2018maskrcnn,
      title={Mask R-CNN}, 
      author={Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
      year={2018},
      eprint={1703.06870},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1703.06870}, 
}

@misc{camera,
  author = {Intel},
  title = {Intel Realsense Depth Camera D435},
  note = {Accessed: 2024-02-02},
  url={https://www.intelrealsense.com/depth-camera-d435/}
}

@misc{Mask3D,
  author = {Schult et al.},
  title = {Mask3D: Mask Transformer for 3D Semantic Instance Segmentation},
  year = {2023},
  howpublished = {\url{https://github.com/JonasSchult/Mask3D}},
  note = {Accessed: 2024-02-02}
}
@misc{colmap2nerf,
  author = { Müller et al. },
  title = {Instant Neural Graphics Primitives},
  howpublished = {\url{https://github.com/NVlabs/instant-ngp/blob/master/scripts/colmap2nerf.py}},
  note = {Accessed: 2024-02-02}
}
@misc{redmon2016lookonceunifiedrealtime,
      title={You Only Look Once: Unified, Real-Time Object Detection}, 
      author={Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
      year={2016},
      eprint={1506.02640},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.02640}, 
}

@misc{jia2021scalingvisualvisionlanguagerepresentation,
      title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, 
      author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
      year={2021},
      eprint={2102.05918},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.05918}, 
}

@misc{ren2016fasterrcnnrealtimeobject,
      title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
      author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
      year={2016},
      eprint={1506.01497},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.01497}, 
}

@ARTICLE{Penny2024-qx,
  title    = "Advancements in {AI} Medical Education: Assessing {ChatGPT's}
              Performance on {USMLE-Style} Questions Across Topics and
              Difficulty Levels",
  author   = "Penny, Parker and Bane, Riley and Riddle, Valerie",
  abstract = "Background AI language models have been shown to achieve a
              passing score on certain imageless diagnostic tests of the USMLE.
              However, they have failed certain specialty-specific
              examinations. This suggests there may be a difference in AI
              ability by medical topic or question difficulty. This study
              evaluates the performance of two versions of ChatGPT, a popular
              language-based AI model, on USMLE-style questions across various
              medical topics. Methods A total of 900 USMLE-style
              multiple-choice questions were equally divided into 18 topics,
              categorized by exam type (step 1 vs. step 2), and copied from
              AMBOSS, a medical learning resource with large question banks.
              Questions that contained images, charts, and tables were excluded
              due to current AI capabilities. The questions were entered into
              ChatGPT-3.5 (version September 25, 2023) and ChatGPT-4 (version
              April 2023) for multiple trials, and performance data were
              recorded. The two AI models were compared against human test
              takers (AMBOSS users) by medical topic and question difficulty.
              Results Chat-GPT-4, AMBOSS users, and Chat-GPT-3.5 had accuracies
              of 71.33\%, 54.38\%, and 46.23\% respectively. When comparing
              models, GPT-4 was a significant improvement demonstrating a 25\%
              greater accuracy and 8\% higher concordance between trials than
              GPT-3 (p<.001). The performance of GPT models was similar between
              step 1 and step 2 content. Both GPT-3.5 and GPT-4 varied
              performance by medical topic (p=.027, p=.002). However, there was
              no clear pattern of variation. Performance for both GPT models
              and AMBOSS users declined as question difficulty increased
              (p<.001). However, the decline in accuracy was less pronounced
              for GPT-4. The accuracy of the GPT models showed less variability
              with question difficulty compared to AMBOSS users, with the
              average drop in accuracy from the easiest to hardest questions
              being 45\% and 62\%, respectively. Discussion ChatGPT-4 shows
              significant improvement over its predecessor, ChatGPT-3.5, in the
              medical education setting. It is the first ChatGPT model to
              surpass human performance on modified AMBOSS USMLE tests. While
              there was variation in performance by medical topic for both
              models, there was no clear pattern of discrepancy. ChatGPT-4's
              improved accuracy, concordance, performance on difficult
              questions, and consistency across topics are promising for its
              reliability and utility for medical learners. Conclusion
              ChatGPT-4's improvements highlight its potential as a valuable
              tool in medical education, surpassing human performance in some
              areas. The lack of a clear performance pattern by medical topic
              suggests that variability is more related to question complexity
              than specific knowledge gaps.",
  journal  = "Cureus",
  volume   =  16,
  number   =  12,
  pages    = "e76309",
  month    =  dec,
  year     =  2024,
  address  = "United States",
  keywords = "ai; artificial intelligence; chat-gpt; medical education; usmle",
  language = "en"
}
@Article{Brin2023,
author={Brin, Dana
and Sorin, Vera
and Vaid, Akhil
and Soroush, Ali
and Glicksberg, Benjamin S.
and Charney, Alexander W.
and Nadkarni, Girish
and Klang, Eyal},
title={Comparing ChatGPT and GPT-4 performance in USMLE soft skill assessments},
journal={Scientific Reports},
year={2023},
month={Oct},
day={01},
volume={13},
number={1},
pages={16492},
issn={2045-2322},
doi={10.1038/s41598-023-43436-9},
url={https://doi.org/10.1038/s41598-023-43436-9}
}

@misc{he2018maskrcnn,
      title={Mask R-CNN}, 
      author={Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
      year={2018},
      eprint={1703.06870},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1703.06870}, 
}

@misc{camera,
  author = {Intel},
  title = {Intel Realsense Depth Camera D435},
  note = {Accessed: 2024-02-02},
  url={https://www.intelrealsense.com/depth-camera-d435/}
}

@misc{Mask3D,
  author = {Schult et al.},
  title = {Mask3D: Mask Transformer for 3D Semantic Instance Segmentation},
  year = {2023},
  howpublished = {\url{https://github.com/JonasSchult/Mask3D}},
  note = {Accessed: 2024-02-02}
}
@misc{colmap2nerf,
  author = { Müller et al. },
  title = {Instant Neural Graphics Primitives},
  howpublished = {\url{https://github.com/NVlabs/instant-ngp/blob/master/scripts/colmap2nerf.py}},
  note = {Accessed: 2024-02-02}
}
@misc{redmon2016lookonceunifiedrealtime,
      title={You Only Look Once: Unified, Real-Time Object Detection}, 
      author={Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
      year={2016},
      eprint={1506.02640},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.02640}, 
}

@misc{jia2021scalingvisualvisionlanguagerepresentation,
      title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, 
      author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
      year={2021},
      eprint={2102.05918},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.05918}, 
}

@misc{ren2016fasterrcnnrealtimeobject,
      title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
      author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
      year={2016},
      eprint={1506.01497},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.01497}, 
}

@ARTICLE{Penny2024-qx,
  title    = "Advancements in {AI} Medical Education: Assessing {ChatGPT's}
              Performance on {USMLE-Style} Questions Across Topics and
              Difficulty Levels",
  author   = "Penny, Parker and Bane, Riley and Riddle, Valerie",
  abstract = "Background AI language models have been shown to achieve a
              passing score on certain imageless diagnostic tests of the USMLE.
              However, they have failed certain specialty-specific
              examinations. This suggests there may be a difference in AI
              ability by medical topic or question difficulty. This study
              evaluates the performance of two versions of ChatGPT, a popular
              language-based AI model, on USMLE-style questions across various
              medical topics. Methods A total of 900 USMLE-style
              multiple-choice questions were equally divided into 18 topics,
              categorized by exam type (step 1 vs. step 2), and copied from
              AMBOSS, a medical learning resource with large question banks.
              Questions that contained images, charts, and tables were excluded
              due to current AI capabilities. The questions were entered into
              ChatGPT-3.5 (version September 25, 2023) and ChatGPT-4 (version
              April 2023) for multiple trials, and performance data were
              recorded. The two AI models were compared against human test
              takers (AMBOSS users) by medical topic and question difficulty.
              Results Chat-GPT-4, AMBOSS users, and Chat-GPT-3.5 had accuracies
              of 71.33\%, 54.38\%, and 46.23\% respectively. When comparing
              models, GPT-4 was a significant improvement demonstrating a 25\%
              greater accuracy and 8\% higher concordance between trials than
              GPT-3 (p<.001). The performance of GPT models was similar between
              step 1 and step 2 content. Both GPT-3.5 and GPT-4 varied
              performance by medical topic (p=.027, p=.002). However, there was
              no clear pattern of variation. Performance for both GPT models
              and AMBOSS users declined as question difficulty increased
              (p<.001). However, the decline in accuracy was less pronounced
              for GPT-4. The accuracy of the GPT models showed less variability
              with question difficulty compared to AMBOSS users, with the
              average drop in accuracy from the easiest to hardest questions
              being 45\% and 62\%, respectively. Discussion ChatGPT-4 shows
              significant improvement over its predecessor, ChatGPT-3.5, in the
              medical education setting. It is the first ChatGPT model to
              surpass human performance on modified AMBOSS USMLE tests. While
              there was variation in performance by medical topic for both
              models, there was no clear pattern of discrepancy. ChatGPT-4's
              improved accuracy, concordance, performance on difficult
              questions, and consistency across topics are promising for its
              reliability and utility for medical learners. Conclusion
              ChatGPT-4's improvements highlight its potential as a valuable
              tool in medical education, surpassing human performance in some
              areas. The lack of a clear performance pattern by medical topic
              suggests that variability is more related to question complexity
              than specific knowledge gaps.",
  journal  = "Cureus",
  volume   =  16,
  number   =  12,
  pages    = "e76309",
  month    =  dec,
  year     =  2024,
  address  = "United States",
  keywords = "ai; artificial intelligence; chat-gpt; medical education; usmle",
  language = "en"
}
  @misc{kirillov2023segment,
      title={Segment Anything}, 
      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
      year={2023},
      eprint={2304.02643},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.02643}, 
}
@misc{bommasani2022opportunitiesrisksfoundationmodels,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}, 
}

@misc{tripathi2019usingscenegraphcontext,
      title={Using Scene Graph Context to Improve Image Generation}, 
      author={Subarna Tripathi and Anahita Bhiwandiwalla and Alexei Bastidas and Hanlin Tang},
      year={2019},
      eprint={1901.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1901.03762}, 
}

@misc{johnson2018imagegenerationscenegraphs,
      title={Image Generation from Scene Graphs}, 
      author={Justin Johnson and Agrim Gupta and Li Fei-Fei},
      year={2018},
      eprint={1804.01622},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1804.01622}, 
}
@misc{zhu2022scenegraphgenerationcomprehensive,
      title={Scene Graph Generation: A Comprehensive Survey}, 
      author={Guangming Zhu and Liang Zhang and Youliang Jiang and Yixuan Dang and Haoran Hou and Peiyi Shen and Mingtao Feng and Xia Zhao and Qiguang Miao and Syed Afaq Ali Shah and Mohammed Bennamoun},
      year={2022},
      eprint={2201.00443},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.00443}, 
}
@Article{Sharma2022,
author={Sharma, Rabi
and Saqib, Muhammad
and Lin, C. T.
and Blumenstein, Michael},
title={A Survey on Object Instance Segmentation},
journal={SN Computer Science},
year={2022},
month={Sep},
day={29},
volume={3},
number={6},
pages={499},
abstract={In recent years, instance segmentation has become a key research area in computer vision. This technology has been applied in varied applications such as robotics, healthcare and intelligent driving. Instance segmentation technology not only detects the location of the object but also marks edges for each single instance, which can solve both object detection and semantic segmentation concurrently. Our survey will give a detail introduction to the instance segmentation technology based on deep learning, reinforcement learning and transformers. Further, we will discuss about its development in this field along with the most common datasets used. We will also focus on different challenges and future development scope for instance segmentation. This technology will provide a strong reference for future researchers in our survey paper.},
issn={2661-8907},
doi={10.1007/s42979-022-01407-3},
url={https://doi.org/10.1007/s42979-022-01407-3}
}

@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}


@article{Zhou2018,
    author    = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},
    title     = {{Open3D}: {A} Modern Library for {3D} Data Processing},
    journal   = {arXiv:1801.09847},
    year      = {2018},
}

@article{Pytorch_Pointnet_Pointnet2,
      author = {Xu Yan},
      title = {Pointnet/Pointnet++ Pytorch},
      url = {https://github.com/yanx27/Pointnet_Pointnet2_pytorch},
      year = {2019}
}

@book{gibson,
 author = {Gibson, J.J.}, 
 year = {2014},
 title = {The Ecological Approach to Visual Perception: Classic Edition (1st ed.).},
 journal = {Psychology Press.}, 
 doi = {https://doi.org/10.4324/9781315740218}
}

@article{mueller2022instant,
    author = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    journal = {ACM Trans. Graph.},
    issue_date = {July 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    pages = {102:1--102:15},
    articleno = {102},
    numpages = {15},
    url = {https://doi.org/10.1145/3528223.3530127},
    doi = {10.1145/3528223.3530127},
    publisher = {ACM},
    address = {New York, NY, USA},
}

@InProceedings{10.1007/978-3-031-20080-9_42,
author="Minderer, Matthias
and Gritsenko, Alexey
and Stone, Austin
and Neumann, Maxim
and Weissenborn, Dirk
and Dosovitskiy, Alexey
and Mahendran, Aravindh
and Arnab, Anurag
and Dehghani, Mostafa
and Shen, Zhuoran
and Wang, Xiao
and Zhai, Xiaohua
and Kipf, Thomas
and Houlsby, Neil",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Simple Open-Vocabulary Object Detection",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="728--755",
abstract="Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub github.com/google-research/scenic/tree/main/scenic/projects/owl{\_}vit.",
isbn="978-3-031-20080-9"
}

@misc{minderer2022simpleopenvocabularyobjectdetection,
      title={Simple Open-Vocabulary Object Detection with Vision Transformers}, 
      author={Matthias Minderer and Alexey Gritsenko and Austin Stone and Maxim Neumann and Dirk Weissenborn and Alexey Dosovitskiy and Aravindh Mahendran and Anurag Arnab and Mostafa Dehghani and Zhuoran Shen and Xiao Wang and Xiaohua Zhai and Thomas Kipf and Neil Houlsby},
      year={2022},
      eprint={2205.06230},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.06230}, 
}

@misc{li2022groundedlanguageimagepretraining,
      title={Grounded Language-Image Pre-training}, 
      author={Liunian Harold Li and Pengchuan Zhang and Haotian Zhang and Jianwei Yang and Chunyuan Li and Yiwu Zhong and Lijuan Wang and Lu Yuan and Lei Zhang and Jenq-Neng Hwang and Kai-Wei Chang and Jianfeng Gao},
      year={2022},
      eprint={2112.03857},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.03857}, 
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@misc{ghiasi2022scalingopenvocabularyimagesegmentation,
      title={Scaling Open-Vocabulary Image Segmentation with Image-Level Labels}, 
      author={Golnaz Ghiasi and Xiuye Gu and Yin Cui and Tsung-Yi Lin},
      year={2022},
      eprint={2112.12143},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.12143}, 
}

@misc{qi2017pointnetdeephierarchicalfeature,
      title={PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space}, 
      author={Charles R. Qi and Li Yi and Hao Su and Leonidas J. Guibas},
      year={2017},
      eprint={1706.02413},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1706.02413}, 
}

@misc{tripathi2019usingscenegraphcontext,
      title={Using Scene Graph Context to Improve Image Generation}, 
      author={Subarna Tripathi and Anahita Bhiwandiwalla and Alexei Bastidas and Hanlin Tang},
      year={2019},
      eprint={1901.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1901.03762}, 
}

@misc{johnson2018imagegenerationscenegraphs,
      title={Image Generation from Scene Graphs}, 
      author={Justin Johnson and Agrim Gupta and Li Fei-Fei},
      year={2018},
      eprint={1804.01622},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1804.01622}, 
}
@misc{zhu2022scenegraphgenerationcomprehensive,
      title={Scene Graph Generation: A Comprehensive Survey}, 
      author={Guangming Zhu and Liang Zhang and Youliang Jiang and Yixuan Dang and Haoran Hou and Peiyi Shen and Mingtao Feng and Xia Zhao and Qiguang Miao and Syed Afaq Ali Shah and Mohammed Bennamoun},
      year={2022},
      eprint={2201.00443},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.00443}, 
}
@Article{Sharma2022,
author={Sharma, Rabi
and Saqib, Muhammad
and Lin, C. T.
and Blumenstein, Michael},
title={A Survey on Object Instance Segmentation},
journal={SN Computer Science},
year={2022},
month={Sep},
day={29},
volume={3},
number={6},
pages={499},
abstract={In recent years, instance segmentation has become a key research area in computer vision. This technology has been applied in varied applications such as robotics, healthcare and intelligent driving. Instance segmentation technology not only detects the location of the object but also marks edges for each single instance, which can solve both object detection and semantic segmentation concurrently. Our survey will give a detail introduction to the instance segmentation technology based on deep learning, reinforcement learning and transformers. Further, we will discuss about its development in this field along with the most common datasets used. We will also focus on different challenges and future development scope for instance segmentation. This technology will provide a strong reference for future researchers in our survey paper.},
issn={2661-8907},
doi={10.1007/s42979-022-01407-3},
url={https://doi.org/10.1007/s42979-022-01407-3}
}

@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}


@article{Zhou2018,
    author    = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},
    title     = {{Open3D}: {A} Modern Library for {3D} Data Processing},
    journal   = {arXiv:1801.09847},
    year      = {2018},
}

@article{Pytorch_Pointnet_Pointnet2,
      author = {Xu Yan},
      title = {Pointnet/Pointnet++ Pytorch},
      url = {https://github.com/yanx27/Pointnet_Pointnet2_pytorch},
      year = {2019}
}

@book{gibson,
 author = {Gibson, J.J.}, 
 year = {2014},
 title = {The Ecological Approach to Visual Perception: Classic Edition (1st ed.).},
 journal = {Psychology Press.}, 
 doi = {https://doi.org/10.4324/9781315740218}
}

@article{mueller2022instant,
    author = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    journal = {ACM Trans. Graph.},
    issue_date = {July 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    pages = {102:1--102:15},
    articleno = {102},
    numpages = {15},
    url = {https://doi.org/10.1145/3528223.3530127},
    doi = {10.1145/3528223.3530127},
    publisher = {ACM},
    address = {New York, NY, USA},
}

@InProceedings{10.1007/978-3-031-20080-9_42,
author="Minderer, Matthias
and Gritsenko, Alexey
and Stone, Austin
and Neumann, Maxim
and Weissenborn, Dirk
and Dosovitskiy, Alexey
and Mahendran, Aravindh
and Arnab, Anurag
and Dehghani, Mostafa
and Shen, Zhuoran
and Wang, Xiao
and Zhai, Xiaohua
and Kipf, Thomas
and Houlsby, Neil",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Simple Open-Vocabulary Object Detection",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="728--755",
abstract="Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub github.com/google-research/scenic/tree/main/scenic/projects/owl{\_}vit.",
isbn="978-3-031-20080-9"
}

@misc{minderer2022simpleopenvocabularyobjectdetection,
      title={Simple Open-Vocabulary Object Detection with Vision Transformers}, 
      author={Matthias Minderer and Alexey Gritsenko and Austin Stone and Maxim Neumann and Dirk Weissenborn and Alexey Dosovitskiy and Aravindh Mahendran and Anurag Arnab and Mostafa Dehghani and Zhuoran Shen and Xiao Wang and Xiaohua Zhai and Thomas Kipf and Neil Houlsby},
      year={2022},
      eprint={2205.06230},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.06230}, 
}

@misc{li2022groundedlanguageimagepretraining,
      title={Grounded Language-Image Pre-training}, 
      author={Liunian Harold Li and Pengchuan Zhang and Haotian Zhang and Jianwei Yang and Chunyuan Li and Yiwu Zhong and Lijuan Wang and Lu Yuan and Lei Zhang and Jenq-Neng Hwang and Kai-Wei Chang and Jianfeng Gao},
      year={2022},
      eprint={2112.03857},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.03857}, 
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@misc{ghiasi2022scalingopenvocabularyimagesegmentation,
      title={Scaling Open-Vocabulary Image Segmentation with Image-Level Labels}, 
      author={Golnaz Ghiasi and Xiuye Gu and Yin Cui and Tsung-Yi Lin},
      year={2022},
      eprint={2112.12143},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.12143}, 
}

@misc{qi2017pointnetdeephierarchicalfeature,
      title={PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space}, 
      author={Charles R. Qi and Li Yi and Hao Su and Leonidas J. Guibas},
      year={2017},
      eprint={1706.02413},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1706.02413}, 
}
