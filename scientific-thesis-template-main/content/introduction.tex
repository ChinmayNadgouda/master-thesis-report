For a future where autonomous robots work independently and precisely complete a given task, a few requirements need to be satisfied. Firstly, the robot needs to successfully identify objects in the vicinity i.e. Perception. Secondly, act on the specific parts of the object keeping in mind the spatial relationships between detected objects i.e. Planning and Execution. Finally, the object detection should take place efficiently and with minimum data needed for training as possible, this will ensure that the robot can detect previously unseen objects on the fly in new and complex environments. For this, open set object detection is a great method of recognising unseen objects on the fly. Although, these requirements have been fulfilled by Concept Graph and others there are still further improvements to be made. For a robot to perform more delicate and specific actions like opening a door, pouring water from a water bottle or cooking vegetables in a pan, the robot needs to build more specific and detailed relationships between an object and its parts. Implementations mentioned above build a scene graph, which is a semantically rich 3d representation of a scene captured by the robot. The scene graph includes point cloud of recognised objects along with their annotated names and relationships between near by objects using edges. But it lacks the semantic details of even finer objects such as door handles, cap of water bottles and many more. This thesis proposes a novel way to extend the information captured by the scene graph in the Concept graph. We plan to also capture part-object relationships and represent them in the form of nodes and edges. To this end, we plan on training a neural network on pre-annotated dataset of objects and their parts. This dataset was formed by merging scenefun3D and Labelmaker ARKIT scenes datasets, which are complements of ARKIT scenes. To motivate our study and get useful results we plan on evaluating our method using two tasks from the three predefined tasks of scenefun3D. The three tasks are functionality segmentation, task defined grounding and (third task) From these we plan on evaluating out method on the first two tasks. For the task functionality segmentation we trained two point cloud networks namely, PointNet++ and Mask3D on our own dataset. To Annotate each object and their parts along with the affordance of each part, we take inspiration from the 8 affordance labels suggested by scenefun3D. Later, we tackle the task, task driven grounding, where we motivate that using scene graph as an intermediate step proves to improve the results of finding the relevant object and part for the given task. Scenefun3D uses the whole scene as an input to network and expects segmentation of the parts in return, however as the ratio between the size of the scene and the parts is very large, the results are not satisfying. Our idea of using a scene graph as an intermediate step provides as an input the object itself which might include the part. Thus bring the large scene as an input to just a subset of the scene i,e the object and eventually the segmented part as an input to the neural network. Excelling in these two tasks will provide a detailed and semantically rich map of the environment to the robot. Such detailed and semantically rich scene graph will enable the robot to perform daily tasks seamlessly. 

\subsection{Milestones of this thesis:}
For the thesis to be successful we have defined a few milestones. The milestones will structure the thesis and will help in evaluating the outcomes.\\

This thesis aims to achieve the following milestones:
\begin{compactenum}[1.]
\item	Implement concept-graph and execute it on synthetic datasets such as, Replica and SceneFun3D.
\item	Get concept graph working for real world dataset captured in SIR lab.
\item	Compare the performance of various foundation models and evaluate their execution time and result accuracy.
\item	Extend the concept graph functionality by implementing part-whole relationship between an object and its parts.
\end{compactenum}
\subsection{Outline of the thesis:}	
The outline for this report will be as follows, this chapter is about Introduction. 
First, \cref{chap:k2} will review the state of the art research, their lacking and the need to materialise the objectives mentioned above. 
Later, \cref{chap:k3} will provide concrete theory to backup the idea of this thesis, we will look at the foundation models and deep learning models
needed to make the implementation possible along with various techniques used to generate, visualise and evaluate the scene graph.
\cref{chap:k4} will briefly explain the research methodology used and the metrics used for evaluation.
\cref{chap:k5} will provide a brief solution stratergy. \cref{chap:k6} will give an insight into the design of the overall implementaiton and the plan to 
execute it.
\cref{chap:k7} will provide details regarding the the implementation of the set objectives along with the setup and requirements of the proposed system. 
Finally, \cref{chap:k8} gives the results obtained after completing all the above-mentioned objectives, the metrics we will be using are 
manual evaluation (qualitative) and average precision over mIoU (AP 50 \% and AP 25 \% ).
